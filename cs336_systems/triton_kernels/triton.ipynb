{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980b604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(X, w):\n",
    "    \"\"\"\n",
    "    Compute MatMul X@w.\n",
    "    \"\"\"\n",
    "    return (X * w).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2d8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\"\"\"\n",
    "In triton, we do not count by # of bytes (like in C).\n",
    "\n",
    "We count by # of elements, ie: id1, id2, id3, ...\n",
    "\"\"\"\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    X_ptr, w_ptr,    # Mat, Vec ptr\n",
    "    out_ptr,         # Output pointer\n",
    "    X_row_stride,    # Number of element to get to the nxt row in X\n",
    "    X_stride_dim,    # Dimension of X's stride\n",
    "    w_stride_dim,    # Dimesnion of w's stride\n",
    "    out_stride_row,  # Number of element to get to the nxt row in output\n",
    "    X_ROW, X_D,     # Shape of the matrix X\n",
    "    TILE_ROW: tl.constexpr, TILE_D: tl.constexpr # Shape of each tile\n",
    "):\n",
    "    # Each instance will compute the weighted sum of a (tile of rows) of x.\n",
    "    # `tl.program_id` gives us a way to check which `thread block` we're running in\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "\n",
    "    # Block pointers give us a way to select from an ND (N-dimensional) region of memory\n",
    "    \"\"\"\n",
    "    The block pointer must know:\n",
    "        - ptr: ptr          The pointer to the first element of the tensor\n",
    "        - shape:Tuple()     The overall shape of the tensor (R, D) to handle out-of-bounds access\n",
    "        - strides:Tuple()   The strides of each dimension (stride_R, stride_D) to use the memory layout properly\n",
    "        - offsets:Tuple()   The ND coordinates of the starting block, i.e., \"offsets\"\n",
    "        - block_shape:Tuple() The block shape to use load/store at a time\n",
    "        - order:Tuple()     The order of the dimensions in memory from major to minor\n",
    "            axes (= np.argsort(strides)) for optimizations\n",
    "\n",
    "    - order: Specify out how the matrix A is stored in the memory. (Contiguous in ROW or in COL?)\n",
    "        \n",
    "        Suppose we have an matrix A, \n",
    "               A = [[ 0,  1,  2,  3],\n",
    "                    [ 4,  5,  6,  7],\n",
    "                    [ 8,  9, 10, 11],\n",
    "                    [12, 13, 14, 15]]\n",
    "\n",
    "        - If order=(0,1) (row-major) → linear memory: 0,1,2,3,4,5,...\n",
    "            Threads along row → coalesced.\n",
    "\n",
    "        - If order=(1,0) (column-major) → linear memory: 0,4,8,12,1,5,9,13,...\n",
    "            Threads along column → coalesced.\n",
    "    \"\"\"\n",
    "\n",
    "    # Row major matrix X in R^(N, D)\n",
    "    \"\"\"\n",
    "    X is loaded as X[TILE_ROW][TILE_D]\n",
    "    \"\"\"\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        X_ptr, # a pointer to X[0, 0]\n",
    "        shape = (X_ROW, X_D), # for boundary check\n",
    "        strides = (X_row_stride, X_stride_dim), # move +1 row / column == +X_row_stride elements / +X_stride_dim elements\n",
    "        offsets = (row_tile_idx * TILE_ROW, 0), # The starting corner (origin) of the current tile; (parallelizes over row tiles)\n",
    "        block_shape = (TILE_ROW, TILE_D),  # The size of the tile.\n",
    "        order = (1,0) # dim1 is more contiguous than dim0; moving in column; row-major layout\n",
    "    )\n",
    "\n",
    "    # Row vector w in R^(D, 1); All Vector's Typing is the SAME\n",
    "    \"\"\"\n",
    "    w is loaded as w[TILE_D]\n",
    "    \"\"\"\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        w_ptr,\n",
    "        shape=(X_D,),\n",
    "        strides=(w_stride_dim,),\n",
    "        offsets=(0,), # The starting corner; (因为是一个Row vec同时我们parallelizes over row tiles，所以是0)\n",
    "        block_shape=(TILE_D,), # The size of the tile.\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    # Output col vector in R^(N, 1)\n",
    "    \"\"\"\n",
    "    output is loaded as o[TILE_D]\n",
    "    \"\"\"\n",
    "    out_block_ptr = tl.make_block_ptr(\n",
    "        out_ptr,\n",
    "        shape=(X_ROW,),\n",
    "        strides=(out_stride_row,),\n",
    "        offsets=(row_tile_idx * TILE_ROW,), # The starting corner (origin) of the current tile; (parallelizes over row tiles)\n",
    "        block_shape=(TILE_ROW,), # The size of the tile.\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    # Initialize a buffer to recieve output\n",
    "    output = tl.zeros((TILE_ROW,), dtype = tl.float32)\n",
    "\n",
    "    # Sweep across columns\n",
    "    for i in range(tl.cdiv(X_D, TILE_D)):\n",
    "        # load blocks ptrs\n",
    "        # 因为有可能 Tile Row/D Size 不能整除 X Row/D Size，所以需要check boundary for Row & D.\n",
    "        # padded with 0 如果 out of boundary\n",
    "        X = tl.load(x_block_ptr, boundary_check=(0,1), padding_option=\"zero\")  \n",
    "        w = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
    "\n",
    "        # Compute Weighted Sum of the block\n",
    "        output += tl.sum(X * w[None, :], axis=1)\n",
    "\n",
    "        # Move the pointers to the next tile\n",
    "        # [[] -> ...\n",
    "        #  [] -> ...    &&  [ [] -> ... ]\n",
    "        #  [] -> ...]\n",
    "        x_block_ptr = x_block_ptr.advance((0, TILE_D)) # Curr += (0, TILE_D)\n",
    "        weight_block_ptr = weight_block_ptr.advance((TILE_D,)) # Curr += (TILE_D,)\n",
    "\n",
    "    # Write the output to the buffer\n",
    "    tl.store(out_block_ptr, output, boundary_check=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5a1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def weighted_sum_backward(\n",
    "    X_ptr, w_ptr,\n",
    "    grad_y_ptr,\n",
    "    grad_X_ptr, grad_tile_w_block_ptr,\n",
    "    stride_X_ROW, stride_X_D,\n",
    "    stride_w_D,\n",
    "    stride_grad_y_ROW,\n",
    "    stride_grad_X_ROW, stride_grad_X_D,\n",
    "    stride_tile_grad_w_ROW, stride_tile_grad_w_D,\n",
    "    X_ROW, X_D,\n",
    "    TILE_ROW, TILE_D\n",
    "):\n",
    "    \"\"\"\n",
    "    Grad X can be parrallized computed. \n",
    "    Grad w can't be fully parrallized computed at once.\n",
    "\n",
    "    Note:\n",
    "        The gradient of weight w is only partially computed in this stage, since \n",
    "        the gradient of w_j requires to sum across all the rows n to compute.\n",
    "    \"\"\"\n",
    "    # Get the current program's tile id\n",
    "    rowtile_idx = tl.program_id(0)\n",
    "    n_rowtile = tl.num_programs(0)\n",
    "\n",
    "    # 1D vector \n",
    "    grad_y_block_ptr = tl.make_block_ptr(\n",
    "        grad_y_ptr,\n",
    "        shape=(X_ROW,),\n",
    "        strides=(stride_grad_y_ROW,),\n",
    "        offsets=(rowtile_idx*TILE_ROW, ),\n",
    "        block_shape=(TILE_ROW,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    # 2D matrix\n",
    "    X_block_ptr = tl.make_block_ptr(\n",
    "        X_ptr,\n",
    "        shape=(X_ROW, X_D),\n",
    "        strides=(stride_X_ROW, stride_X_D),\n",
    "        offsets=(rowtile_idx*TILE_ROW, 0),\n",
    "        block_shape=(TILE_ROW, TILE_D),\n",
    "        order=(1, 0)\n",
    "    )\n",
    "\n",
    "    # 1D vector\n",
    "    w_block_ptr = tl.make_block_ptr(\n",
    "        w_ptr,\n",
    "        shape=(X_D,),\n",
    "        strides=(stride_w_D,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(TILE_D,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    # 2D matrix\n",
    "    grad_X_block_ptr = tl.make_block_ptr(\n",
    "        grad_X_ptr,\n",
    "        shape=(X_ROW, X_D),\n",
    "        strides=(stride_grad_X_ROW, stride_grad_X_D),\n",
    "        offsets=(rowtile_idx*TILE_ROW, 0),\n",
    "        block_shape=(TILE_ROW, TILE_D),\n",
    "        order=(1,0)\n",
    "    )\n",
    "\n",
    "    # A partially computed gradient, not sum() reduced yet\n",
    "    grad_tile_w_block_ptr = tl.make_block_ptr(\n",
    "        grad_tile_w_block_ptr,\n",
    "        shape=(n_rowtile, X_D,),\n",
    "        strides=(stride_tile_grad_w_ROW, stride_tile_grad_w_D),\n",
    "        offsets=(rowtile_idx, 0),\n",
    "        block_shape=(1, TILE_D),\n",
    "        order=(1,0)\n",
    "    )\n",
    "\n",
    "    # Sweeping across COLUMNS\n",
    "    for i in range(tl.cdiv(X_D, TILE_D)):\n",
    "        # Load all the blocks\n",
    "        grad_y = tl.load(grad_y_block_ptr, boundary_check=(0,), padding_option=\"zero\") # (X_D,)\n",
    "        w_Tile = tl.load(w_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (TILE_D)\n",
    "        X_Tile = tl.load(X_block_ptr, boundary_check=(0,1))\n",
    "\n",
    "        # Compute dL/dX = outer_prod(grad_y, w) = [nx1][1xD]\n",
    "        grad_X = grad_y[:,None] * w_Tile[None, :]\n",
    "        tl.store(grad_X_block_ptr, grad_X, boundary_check=(0, 1))\n",
    "        \n",
    "        # Compute one tile of dL/dw_j = Sum_{k = tiles rows}(grad_y_k * x_kj); [... [x_k(j), ..., x_k(j+TILE_SIZE)] ...]\n",
    "        grad_tile_w = tl.sum(X_Tile * grad_y[:, None], axis = 0)\n",
    "        tl.store(grad_tile_w_block_ptr, grad_tile_w, boundary_check=(0,1))\n",
    "\n",
    "        # Move the pointer to the next tile along D\n",
    "        X_block_ptr = X_block_ptr.advance((0, TILE_D))\n",
    "        w_block_ptr = w_block_ptr.advance((TILE_D,))\n",
    "        grad_tile_w_block_ptr = grad_tile_w_block_ptr.advance((0, TILE_D))\n",
    "        grad_X_block_ptr = grad_X_block_ptr.advance((0, TILE_D))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02969b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "import einops\n",
    "from einops import rearrange\n",
    "from triton import cdiv\n",
    "\n",
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, weight):\n",
    "        X_D, out_dim = X.shape[-1], X.shape[:-1] # `last dim`, `all dim except for the last dim`\n",
    "        input_shape = X.shape\n",
    "        \n",
    "        # Reshape X into 2D\n",
    "        X = rearrange(X, \"... d -> (...) d\")\n",
    "\n",
    "        # ctx is \n",
    "        ctx.save_for_backward(X, weight)\n",
    "\n",
    "        # Prior checks \n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == X_D, \"ValueError: Matrix Vector Dimension Mismatch\"\n",
    "        assert X.is_cuda and weight.is_cuda, \"TypeError: Expect CUDA Tensors, got other\"\n",
    "        assert X.is_contiguous(), \"TypeError: Expect a Contiguous Tensor X\"\n",
    "\n",
    "        # Define triton kernel constants\n",
    "        ctx.TILE_ROW = 16 # Fixed the # of rows in Tile\n",
    "        ctx.TILE_D = triton.next_power_of_2(X_D) # Fixed the Tile size to be power of 2, ie: 1024, 2048, ....\n",
    "        ctx.input_shape = input_shape \n",
    "\n",
    "        # Need to initialize empty result tensor. Note that these elements are not necessarily Output.\n",
    "        y = torch.empty(out_dim, device=X.device)\n",
    "\n",
    "        # Launch kernel\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(cdiv(n_rows, ctx.TILE_ROW),)]( # define launch grid\n",
    "            X, weight,\n",
    "            y,\n",
    "            X.stride(0), X.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            X_ROW=n_rows, X_D=X_D,\n",
    "            TILE_ROW=ctx.TILE_ROW, TILE_D=ctx.TILE_D\n",
    "        )\n",
    "\n",
    "        return y.view(input_shape[:-1])\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        \"\"\"\n",
    "        Return the gradient of the two variables stored in ctx object\n",
    "        \"\"\"\n",
    "        X, w = ctx.saved_tensors\n",
    "        TILE_ROW = ctx.TILE_ROW\n",
    "        TILE_D = ctx.TILE_D\n",
    "        X_ROW, X_D = X.shape\n",
    "\n",
    "        # Compute the gradient X and gradient tile w\n",
    "        n_TILEs = cdiv(X_ROW, TILE_ROW)\n",
    "        grad_tile_w_buffer = torch.empty((n_TILEs, X_D), device=X.device, dtype=X.dtype)\n",
    "        grad_X_buffer = torch.empty_like(X)\n",
    "\n",
    "        weighted_sum_backward[(n_TILEs,)](\n",
    "            X, w,\n",
    "            grad_y,\n",
    "            grad_X_buffer, grad_tile_w_buffer,\n",
    "            X.stride(0), X.stride(1),\n",
    "            w.stride(0),\n",
    "            grad_y.stride(0),\n",
    "            grad_X_buffer.stride(0), grad_X_buffer.stride(1),\n",
    "            grad_tile_w_buffer.stride(0), grad_tile_w_buffer.stride(1),\n",
    "            X_ROW=X_ROW, X_D=X_D,\n",
    "            TILE_ROW=TILE_ROW, TILE_D=TILE_D\n",
    "        )\n",
    "        \n",
    "        # Reduce grad_tile_w to full grad_w\n",
    "        grad_w = grad_tile_w_buffer.sum(dim=0)\n",
    "\n",
    "        # Reshape back X tensor shape\n",
    "        grad_X = grad_X_buffer.view(ctx.input_shape)\n",
    "        return grad_X, grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c2f720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
       "       grad_fn=<WeightedSumFuncBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn((10,5), device=\"cuda\", requires_grad=True)\n",
    "w = torch.zeros((5,), device=\"cuda\", requires_grad=True)\n",
    "\n",
    "fn = WeightedSumFunc.apply(X,w)\n",
    "fn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
