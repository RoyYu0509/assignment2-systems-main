{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "980b604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(X, w):\n",
    "    \"\"\"\n",
    "    Compute MatMul X@w.\n",
    "    \"\"\"\n",
    "    return (X * w).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\"\"\"\n",
    "In triton, we do not count by # of bytes (like in C).\n",
    "\n",
    "We count by # of elements, ie: id1, id2, id3, ...\n",
    "\"\"\"\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    X_ptr, w_ptr,    # Mat, Vec ptr\n",
    "    out_ptr,         # Output pointer\n",
    "    X_row_stride,    # Number of element to get to the nxt row in X\n",
    "    X_stride_dim,    # Dimension of X's stride\n",
    "    w_stride_dim,    # Dimesnion of w's stride\n",
    "    out_stride_row,  # Number of element to get to the nxt row in output\n",
    "    X_ROW, X_D,     # Shape of the matrix X\n",
    "    TILE_ROW: tl.constexpr, TILE_D: tl.constexpr # Shape of each tile\n",
    "):\n",
    "    # Each instance will compute the weighted sum of a (tile of rows) of x.\n",
    "    # `tl.program_id` gives us a way to check which `thread block` we're running in\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "\n",
    "    # Block pointers give us a way to select from an ND (N-dimensional) region of memory\n",
    "    \"\"\"\n",
    "    The block pointer must know:\n",
    "        - ptr: ptr          The pointer to the first element of the tensor\n",
    "        - shape:Tuple()     The overall shape of the tensor (R, D) to handle out-of-bounds access\n",
    "        - strides:Tuple()   The strides of each dimension (stride_R, stride_D) to use the memory layout properly\n",
    "        - offsets:Tuple()   The ND coordinates of the starting block, i.e., \"offsets\"\n",
    "        - block_shape:Tuple() The block shape to use load/store at a time\n",
    "        - order:Tuple()     The order of the dimensions in memory from major to minor\n",
    "            axes (= np.argsort(strides)) for optimizations\n",
    "\n",
    "    - order: Specify out how the matrix A is stored in the memory. (Contiguous in ROW or in COL?)\n",
    "        \n",
    "        Suppose we have an matrix A, \n",
    "               A = [[ 0,  1,  2,  3],\n",
    "                    [ 4,  5,  6,  7],\n",
    "                    [ 8,  9, 10, 11],\n",
    "                    [12, 13, 14, 15]]\n",
    "\n",
    "        - If order=(0,1) (row-major) → linear memory: 0,1,2,3,4,5,...\n",
    "            Threads along row → coalesced.\n",
    "\n",
    "        - If order=(1,0) (column-major) → linear memory: 0,4,8,12,1,5,9,13,...\n",
    "            Threads along column → coalesced.\n",
    "    \"\"\"\n",
    "\n",
    "    # Row major matrix X in R^(N, D)\n",
    "    \"\"\"\n",
    "    X is loaded as X[TILE_ROW][TILE_D]\n",
    "    \"\"\"\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        X_ptr, # a pointer to X[0, 0]\n",
    "        shape = (X_ROW, X_D), # for boundary check\n",
    "        strides = (X_row_stride, X_stride_dim), # move +1 row / column == +X_row_stride elements / +X_stride_dim elements\n",
    "        offsets = (row_tile_idx * TILE_ROW, 0), # The starting corner (origin) of the current tile; (parallelizes over row tiles)\n",
    "        block_shape = (TILE_ROW, TILE_D),  # The size of the tile.\n",
    "        order = (1,0) # dim1 is more contiguous than dim0; moving in column; row-major layout\n",
    "    )\n",
    "\n",
    "    # Row vector w in R^(D, 1); All Vector's Typing is the SAME\n",
    "    \"\"\"\n",
    "    w is loaded as w[TILE_D]\n",
    "    \"\"\"\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        w_ptr,\n",
    "        shape=(X_D,),\n",
    "        strides=(w_stride_dim,),\n",
    "        offsets=(0,), # The starting corner; (因为是一个Row vec同时我们parallelizes over row tiles，所以是0)\n",
    "        block_shape=(TILE_D,), # The size of the tile.\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    # Output col vector in R^(N, 1)\n",
    "    \"\"\"\n",
    "    output is loaded as o[TILE_D]\n",
    "    \"\"\"\n",
    "    out_block_ptr = tl.make_block_ptr(\n",
    "        out_ptr,\n",
    "        shape=(X_ROW,),\n",
    "        strides=(out_stride_row,),\n",
    "        offsets=(row_tile_idx * TILE_ROW,), # The starting corner (origin) of the current tile; (parallelizes over row tiles)\n",
    "        block_shape=(TILE_ROW,), # The size of the tile.\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    # Initialize a buffer to recieve output\n",
    "    output = tl.zeros((TILE_ROW), dtype = tl.float32)\n",
    "\n",
    "    # Sweep across columns\n",
    "    for i in range(tl.cdiv(X_D, TILE_D)):\n",
    "        # load blocks ptrs\n",
    "        # 因为有可能 Tile Row/D Size 不能整除 X Row/D Size，所以需要check boundary for Row & D.\n",
    "        # padded with 0 如果 out of boundary\n",
    "        X = tl.load(x_block_ptr, boundary_check=(0,1), padding_option=\"zero\")  \n",
    "        w = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
    "\n",
    "        # Compute Weighted Sum of the block\n",
    "        output += tl.sum(X * w[None, :], axis=1)\n",
    "\n",
    "        # Move the pointers to the next tile\n",
    "        # [[] -> ...\n",
    "        #  [] -> ...    &&  [ [] -> ... ]\n",
    "        #  [] -> ...]\n",
    "        x_block_ptr = x_block_ptr.advance((0, TILE_D)) # Curr += (0, TILE_D)\n",
    "        weight_block_ptr = weight_block_ptr.advance((TILE_D,)) # Curr += (TILE_D,)\n",
    "\n",
    "    # Write the output to the buffer\n",
    "    tl.store(out_block_ptr, output, boundary_check=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d5a1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def weighted_sum_backward(\n",
    "    X_ptr, w_ptr,\n",
    "    grad_y_ptr,\n",
    "    grad_X_ptr, grad_tile_w_block_ptr,\n",
    "    stride_X_ROW, stride_X_D,\n",
    "    stride_w_D,\n",
    "    stride_grad_y_ROW,\n",
    "    stride_grad_X_ROW, stride_grad_X_D,\n",
    "    stride_tile_grad_w_ROW, stride_tile_grad_w_D,\n",
    "    X_ROW, X_D,\n",
    "    TILE_ROW, TILE_D\n",
    "):\n",
    "    \"\"\"\n",
    "    Grad X can be parrallized computed. \n",
    "    Grad w can't be fully parrallized computed at once.\n",
    "\n",
    "    Note:\n",
    "        The gradient of weight w is only partially computed in this stage, since \n",
    "        the gradient of w_j requires to sum across all the rows n to compute.\n",
    "    \"\"\"\n",
    "    # Get the current program's tile id\n",
    "    rowtile_idx = tl.program_id(0)\n",
    "    n_rowtile = tl.num_programs(0)\n",
    "\n",
    "    # 1D vector \n",
    "    grad_y_block_ptr = tl.make_block_ptr(\n",
    "        grad_y_ptr,\n",
    "        shape=(X_ROW,),\n",
    "        strides=(stride_grad_y_ROW,),\n",
    "        offsets=(rowtile_idx*TILE_ROW, ),\n",
    "        block_shape=(TILE_ROW,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    # 2D matrix\n",
    "    X_block_ptr = tl.make_block_ptr(\n",
    "        X_ptr,\n",
    "        shape=(X_ROW, X_D),\n",
    "        strides=(stride_X_ROW, stride_X_D),\n",
    "        offsets=(rowtile_idx*TILE_ROW, 0),\n",
    "        block_shape=(TILE_ROW, TILE_D),\n",
    "        order=(1, 0)\n",
    "    )\n",
    "\n",
    "    # 1D vector\n",
    "    w_block_ptr = tl.make_block_ptr(\n",
    "        w_ptr,\n",
    "        shape=(X_D,),\n",
    "        strides=(stride_w_D,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(TILE_D,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    # 2D matrix\n",
    "    grad_X_block_ptr = tl.make_block_ptr(\n",
    "        grad_X_ptr,\n",
    "        shape=(X_ROW, X_D),\n",
    "        strides=(stride_grad_X_ROW, stride_grad_X_D),\n",
    "        offsets=(rowtile_idx*TILE_ROW, 0),\n",
    "        block_shape=(TILE_ROW, TILE_D),\n",
    "        order=(1,0)\n",
    "    )\n",
    "\n",
    "    # A partially computed gradient, not sum() reduced yet\n",
    "    grad_tile_w_block_ptr = tl.make_block_ptr(\n",
    "        grad_tile_w_block_ptr,\n",
    "        shape=(n_rowtile, X_D,),\n",
    "        strides=(stride_tile_grad_w_ROW, stride_tile_grad_w_D),\n",
    "        offsets=(rowtile_idx, 0),\n",
    "        block_shape=(1, TILE_D),\n",
    "        order=(1,0)\n",
    "    )\n",
    "\n",
    "    # Sweeping across COLUMNS\n",
    "    for i in range(tl.cdiv(X_D, TILE_D)):\n",
    "        # Load all the blocks\n",
    "        grad_y = tl.load(grad_y_block_ptr, boundary_check=(0,), padding_option=\"zero\") # (X_D,)\n",
    "        w_Tile = tl.load(w_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (TILE_D)\n",
    "        X_Tile = tl.load(X_block_ptr, boundary_check=(0,1))\n",
    "\n",
    "        # Compute dL/dX = outer_prod(grad_y, w) = [nx1][1xD]\n",
    "        grad_X = grad_y[:,None] * w_Tile[None, :]\n",
    "        tl.store(grad_X_block_ptr, grad_X, boundary_check=(0, 1))\n",
    "        \n",
    "        # Compute one tile of dL/dw_j = Sum_{k = tiles rows}(grad_y_k * x_kj); [... [x_k(j), ..., x_k(j+TILE_SIZE)] ...]\n",
    "        grad_tile_w = tl.sum(X_Tile * grad_y[:, None], axis = 0)\n",
    "        tl.store(grad_tile_w_block_ptr, grad_tile_w, boundary_check=(0,1))\n",
    "\n",
    "        # Move the pointer to the next tile along D\n",
    "        X_block_ptr = X_block_ptr.advance((0, TILE_D))\n",
    "        w_block_ptr = w_block_ptr.advance((TILE_D,))\n",
    "        grad_tile_w_block_ptr = grad_tile_w_block_ptr.advance((0, TILE_D))\n",
    "        grad_X_block_ptr = grad_X_block_ptr.advance((0, TILE_D))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02969b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "import einops\n",
    "from einops import rearrange\n",
    "from triton import cdiv\n",
    "\n",
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, weight):\n",
    "        X_D, out_dim = X.shape[-1], X.shape[:-1] # `last dim`, `all dim except for the last dim`\n",
    "        input_shape = X.shape\n",
    "        \n",
    "        # Reshape X into 2D\n",
    "        X = rearrange(X, \"... d -> (...) d\")\n",
    "\n",
    "        # ctx is \n",
    "        ctx.save_for_backward(X, weight)\n",
    "\n",
    "        # Prior checks \n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == X_D, \"ValueError: Matrix Vector Dimension Mismatch\"\n",
    "        assert X.is_cuda and weight.is_cuda, \"TypeError: Expect CUDA Tensors, got other\"\n",
    "        assert X.is_contiguous(), \"TypeError: Expect a Contiguous Tensor X\"\n",
    "\n",
    "        # Define triton kernel constants\n",
    "        ctx.TILE_ROW = 16 # Fixed the # of rows in Tile\n",
    "        ctx.TILE_D = triton.next_power_of_2(X_D) # Fixed the Tile size to be power of 2, ie: 1024, 2048, ....\n",
    "        ctx.input_shape = input_shape \n",
    "\n",
    "        # Need to initialize empty result tensor. Note that these elements are not necessarily Output.\n",
    "        y = torch.empty(out_dim, device=X.device)\n",
    "\n",
    "        # Launch kernel\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(cdiv(n_rows, ctx.TILE_ROW),)]( # define launch grid\n",
    "            X, weight,\n",
    "            y,\n",
    "            X.stride(0), X.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            X_ROW=n_rows, X_D=X_D,\n",
    "            TILE_ROW=ctx.TILE_ROW, TILE_D=ctx.TILE_D\n",
    "        )\n",
    "\n",
    "        return y.view(input_shape[:-1])\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        \"\"\"\n",
    "        Return the gradient of the two variables stored in ctx object\n",
    "        \"\"\"\n",
    "        X, w = ctx.saved_tensors\n",
    "        TILE_ROW = ctx.TILE_ROW\n",
    "        TILE_D = ctx.TILE_D\n",
    "        X_ROW, X_D = X.shape\n",
    "\n",
    "        # Compute the gradient X and gradient tile w\n",
    "        n_TILEs = cdiv(X_ROW, TILE_ROW)\n",
    "        grad_tile_w_buffer = torch.empty((n_TILEs, X_D), device=X.device, dtype=X.dtype)\n",
    "        grad_X_buffer = torch.empty_like(X)\n",
    "\n",
    "        weighted_sum_backward[(n_TILEs,)](\n",
    "            X, w,\n",
    "            grad_y,\n",
    "            grad_X_buffer, grad_tile_w_buffer,\n",
    "            X.stride(0), X.stride(1),\n",
    "            w.stride(0),\n",
    "            grad_y.stride(0),\n",
    "            grad_X_buffer.stride(0), grad_X_buffer.stride(1),\n",
    "            grad_tile_w_buffer.stride(0), grad_tile_w_buffer.stride(1),\n",
    "            X_ROW=X_ROW, X_D=X_D,\n",
    "            TILE_ROW=TILE_ROW, TILE_D=TILE_D\n",
    "        )\n",
    "        \n",
    "        # Reduce grad_tile_w to full grad_w\n",
    "        grad_w = grad_tile_w_buffer.sum(dim=0)\n",
    "\n",
    "        # Reshape back X tensor shape\n",
    "        grad_X = grad_X_buffer.view(ctx.input_shape)\n",
    "        return grad_X, grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90c2f720",
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 81:13:\n    \"\"\"\n    out_block_ptr = tl.make_block_ptr(\n        out_ptr,\n        shape=(X_ROW,),\n        strides=(out_stride_row,),\n        offsets=(row_tile_idx * TILE_ROW,), # The starting corner (origin) of the current tile; (parallelizes over row tiles)\n        block_shape=(TILE_ROW,), # The size of the tile.\n        order=(0,),\n    )\n\n    # Initialize a buffer to recieve output\n    output = tl.zeros((TILE_ROW))\n             ^\nTypeError(\"zeros() missing 1 required positional argument: 'dtype'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCompilationError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m X = torch.randn((\u001b[32m10\u001b[39m,\u001b[32m5\u001b[39m), device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m w = torch.randn((\u001b[32m5\u001b[39m,), device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m fn = \u001b[43mWeightedSumFunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment2-systems-main/.venv/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mWeightedSumFunc.forward\u001b[39m\u001b[34m(ctx, X, weight)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Launch kernel\u001b[39;00m\n\u001b[32m     33\u001b[39m n_rows = y.numel()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mweighted_sum_fwd\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTILE_ROW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# define launch grid\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_ROW\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_D\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTILE_ROW\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTILE_ROW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTILE_D\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTILE_D\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y.view(input_shape[:-\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment2-systems-main/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:330\u001b[39m, in \u001b[36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) -> T:\n\u001b[32m    325\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[33;03m    memorizes the grid.\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment2-systems-main/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:623\u001b[39m, in \u001b[36mJITFunction.run\u001b[39m\u001b[34m(self, grid, warmup, *args, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[32m    622\u001b[39m src = \u001b[38;5;28mself\u001b[39m.ASTSource(\u001b[38;5;28mself\u001b[39m, signature, constants, configs[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[38;5;28mself\u001b[39m.cache[device][key] = kernel\n\u001b[32m    629\u001b[39m \u001b[38;5;28mself\u001b[39m._call_hook(key, signature, device, constants, options, configs, warmup, before=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment2-systems-main/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py:273\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(src, target, options)\u001b[39m\n\u001b[32m    271\u001b[39m module_map = backend.get_module_map()\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     module = \u001b[43msrc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    275\u001b[39m     filter_traceback(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment2-systems-main/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py:100\u001b[39m, in \u001b[36mASTSource.make_ir\u001b[39m\u001b[34m(self, options, codegen_fns, module_map, context)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, codegen_fns, module_map, context):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mCompilationError\u001b[39m: at 81:13:\n    \"\"\"\n    out_block_ptr = tl.make_block_ptr(\n        out_ptr,\n        shape=(X_ROW,),\n        strides=(out_stride_row,),\n        offsets=(row_tile_idx * TILE_ROW,), # The starting corner (origin) of the current tile; (parallelizes over row tiles)\n        block_shape=(TILE_ROW,), # The size of the tile.\n        order=(0,),\n    )\n\n    # Initialize a buffer to recieve output\n    output = tl.zeros((TILE_ROW))\n             ^\nTypeError(\"zeros() missing 1 required positional argument: 'dtype'\")"
     ]
    }
   ],
   "source": [
    "X = torch.randn((10,5), device=\"cuda\")\n",
    "w = torch.randn((5,), device=\"cuda\")\n",
    "\n",
    "fn = WeightedSumFunc.apply(X,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686de14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c701b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
