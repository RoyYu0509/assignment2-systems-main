{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(X, w):\n",
    "    \"\"\"\n",
    "    Compute MatMul X@w.\n",
    "    \"\"\"\n",
    "    return (X * w).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\"\"\"\n",
    "In triton, we do not count by # of bytes (like in C).\n",
    "\n",
    "We count by # of elements, ie: id1, id2, id3, ...\n",
    "\"\"\"\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    X_ptr, w_ptr,    # Mat, Vec ptr\n",
    "    out_ptr,         # Output pointer\n",
    "    X_row_stride,    # Number of element to get to the nxt row in X\n",
    "    X_stride_dim,    # Dimension of X's stride\n",
    "    w_stride_dim,    # Dimesnion of w's stride\n",
    "    out_stride_row,  # Number of element to get to the nxt row in output\n",
    "    X_ROW, X_D,     # Shape of the matrix X\n",
    "    TILE_ROWS_SIZE, TILE_D_SIZE # Shape of each tile\n",
    "):\n",
    "    # Each instance will compute the weighted sum of a (tile of rows) of x.\n",
    "    # `tl.program_id` gives us a way to check which `thread block` we're running in\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "\n",
    "    # Block pointers give us a way to select from an ND (N-dimensional) region of memory\n",
    "    \"\"\"\n",
    "    The block pointer must know:\n",
    "        - ptr: ptr          The pointer to the first element of the tensor\n",
    "        - shape:Tuple()     The overall shape of the tensor (R, D) to handle out-of-bounds access\n",
    "        - strides:Tuple()   The strides of each dimension (stride_R, stride_D) to use the memory layout properly\n",
    "        - offsets:Tuple()   The ND coordinates of the starting block, i.e., \"offsets\"\n",
    "        - block_shape:Tuple() The block shape to use load/store at a time\n",
    "        - order:Tuple()     The order of the dimensions in memory from major to minor\n",
    "            axes (= np.argsort(strides)) for optimizations\n",
    "\n",
    "    - order: Specify out how the matrix A is stored in the memory. (Contiguous in ROW or in COL?)\n",
    "        \n",
    "        Suppose we have an matrix A, \n",
    "               A = [[ 0,  1,  2,  3],\n",
    "                    [ 4,  5,  6,  7],\n",
    "                    [ 8,  9, 10, 11],\n",
    "                    [12, 13, 14, 15]]\n",
    "\n",
    "        - If order=(0,1) (row-major) → linear memory: 0,1,2,3,4,5,...\n",
    "            Threads along row → coalesced.\n",
    "\n",
    "        - If order=(1,0) (column-major) → linear memory: 0,4,8,12,1,5,9,13,...\n",
    "            Threads along column → coalesced.\n",
    "    \"\"\"\n",
    "\n",
    "    # Row major matrix X in R^(N, D)\n",
    "    \"\"\"\n",
    "    X is loaded as X[TILE_ROWS_SIZE][TILE_D_SIZE]\n",
    "    \"\"\"\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        X_ptr, # a pointer to X[0, 0]\n",
    "        shape = (X_ROW, X_D), # for boundary check\n",
    "        strides = (X_row_stride, X_stride_dim), # move +1 row / column == +X_row_stride elements / +X_stride_dim elements\n",
    "        offsets = (row_tile_idx * TILE_ROWS_SIZE, 0), # The starting corner (origin) of the current tile; (parallelizes over row tiles)\n",
    "        block_shape = (TILE_ROWS_SIZE, TILE_D_SIZE),  # The size of the tile.\n",
    "        order = (1,0) # dim1 is more contiguous than dim0; moving in column; row-major layout\n",
    "    )\n",
    "\n",
    "    # Row vector w in R^(D, 1); All Vector's Typing is the SAME\n",
    "    \"\"\"\n",
    "    w is loaded as w[TILE_D_SIZE]\n",
    "    \"\"\"\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        w_ptr,\n",
    "        shape=(X_D,),\n",
    "        strides=(w_stride_dim,),\n",
    "        offsets=(0,), # The starting corner; (因为是一个Row vec同时我们parallelizes over row tiles，所以是0)\n",
    "        block_shape=(TILE_D_SIZE,), # The size of the tile.\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    # Output col vector in R^(N, 1)\n",
    "    \"\"\"\n",
    "    output is loaded as o[TILE_D_SIZE]\n",
    "    \"\"\"\n",
    "    out_block_ptr = tl.make_block_ptr(\n",
    "        out_ptr,\n",
    "        shape=(X_ROW,),\n",
    "        strides=(out_stride_row),\n",
    "        offsets=(row_tile_idx * TILE_ROWS_SIZE,), # The starting corner (origin) of the current tile; (parallelizes over row tiles)\n",
    "        block_shape=(TILE_ROWS_SIZE,), # The size of the tile.\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    # Initialize a buffer to recieve output\n",
    "    output = tl.zeros((TILE_ROWS_SIZE,))\n",
    "\n",
    "    # Sweep across columns\n",
    "    for i in range(tl.cdiv(X_D, TILE_D_SIZE)):\n",
    "        # load blocks ptrs\n",
    "        # 因为有可能 Tile Row/D Size 不能整除 X Row/D Size，所以需要check boundary for Row & D.\n",
    "        # padded with 0 如果 out of boundary\n",
    "        X_ROW = tl.load(x_block_ptr, boundary_check=(0,1), padding_option=\"zero\")  \n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
    "\n",
    "        # Compute Weighted Sum of the block\n",
    "        output += tl.sum(X_ROW * weight[None, :], axis=1)\n",
    "\n",
    "        # Move the pointers to the next tile\n",
    "        # [[] -> ...\n",
    "        #  [] -> ...    &&  [ [] -> ... ]\n",
    "        #  [] -> ...]\n",
    "        x_block_ptr = x_block_ptr.advance((0, TILE_D_SIZE)) # Curr += (0, TILE_D_SIZE)\n",
    "        weight_block_ptr = weight_block_ptr((TILE_D_SIZE,)) # Curr += (TILE_D_SIZE,)\n",
    "\n",
    "    # Write the output to the buffer\n",
    "    tl.store(out_block_ptr, output, boundary_check=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c48720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from einops import rearrange\n",
    "from triton import cdiv\n",
    "\n",
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, weight):\n",
    "        X_D, out_dim = X.shape[-1], X.shape[:-1] # `last dim`, `all dim except for the last dim`\n",
    "        input_shape = X.shape\n",
    "        \n",
    "        # Reshape X into 2D\n",
    "        X = rearrange(X, \"... d -> (...) d\")\n",
    "\n",
    "        # ctx is \n",
    "        ctx.save_for_backward(X, weight)\n",
    "\n",
    "        # Prior checks \n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == X_D, \"ValueError: Matrix Vector Dimension Mismatch\"\n",
    "        assert X.is_cuda and weight.is_cuda, \"TypeError: Expect CUDA Tensors, got other\"\n",
    "        assert X.is_contiguous(), \"TypeError: Expect a Contiguous Tensor X\"\n",
    "\n",
    "        # Define triton kernel constants\n",
    "        ctx.TILE_D_SIZE = triton.next_power_of_2(X_D) # Fixed the Tile size to be power of 2, ie: 1024, 2048, ....\n",
    "        ctx.TILE_ROW_SIZE = 16 # Fixed the # of rows in Tile\n",
    "        ctx.input_shape = input_shape \n",
    "\n",
    "        # Need to initialize empty result tensor. Note that these elements are not necessarily Output.\n",
    "        y = torch.empty(out_dim, device=X.device)\n",
    "\n",
    "        # Launch kernel\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(cdiv(n_rows, ctx.TILE_ROW_SIZE),)]( # define launch grid\n",
    "            X, weight,\n",
    "            y,\n",
    "            X.stride(0), X.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            X_ROW=n_rows, X_D=X_D,\n",
    "            TILE_ROWS_SIZE=ctx.TILE_ROWS_SIZE, TILE_D_SIZE=ctx.TILE_D_SIZE\n",
    "        )\n",
    "\n",
    "        return y.view(input_shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a1713",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'triton' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129m@triton\u001b[39m.jit\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mweighted_sum_backward\u001b[39m(\n\u001b[32m      3\u001b[39m     X_ptr, w_ptr,\n\u001b[32m      4\u001b[39m     grad_y_ptr,\n\u001b[32m      5\u001b[39m     grad_X_ptr, tile_grad_w_ptr,\n\u001b[32m      6\u001b[39m     stride_X_ROW, stride_X_D,\n\u001b[32m      7\u001b[39m     stride_w_D,\n\u001b[32m      8\u001b[39m     stride_grad_y_ROW,\n\u001b[32m      9\u001b[39m     stride_grad_X_ROW, stride_grad_X_D,\n\u001b[32m     10\u001b[39m     stride_tile_grad_w_ROW, stride_tile_grad_w_D,\n\u001b[32m     11\u001b[39m     X_ROW, X_D,\n\u001b[32m     12\u001b[39m     TILE_ROW, TILE_D\n\u001b[32m     13\u001b[39m ):\n\u001b[32m     14\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    The gradient of weight w is only partially computed in this stage. Since the gradient of w_j\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    requires to sum across all the rows n to compute. \u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Get the current program's tile id\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'triton' is not defined"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def weighted_sum_backward(\n",
    "    X_ptr, w_ptr,\n",
    "    grad_y_ptr,\n",
    "    grad_X_ptr, tile_grad_w_ptr,\n",
    "    stride_X_ROW, stride_X_D,\n",
    "    stride_w_D,\n",
    "    stride_grad_y_ROW,\n",
    "    stride_grad_X_ROW, stride_grad_X_D,\n",
    "    stride_tile_grad_w_ROW, stride_tile_grad_w_D,\n",
    "    X_ROW, X_D,\n",
    "    TILE_ROW, TILE_D\n",
    "):\n",
    "    \"\"\"\n",
    "    Grad X can parrallized \n",
    "    Grad w can't fully parrallized computed at once.\n",
    "\n",
    "    Note:\n",
    "        The gradient of weight w is only partially computed in this stage, since \n",
    "        the gradient of w_j requires to sum across all the rows n to compute.\n",
    "    \"\"\"\n",
    "    # Get the current program's tile id\n",
    "    rowtile_idx = tl.program_id(0)\n",
    "    n_rowtile = tl.num_programs(0)\n",
    "\n",
    "    # 1D vector \n",
    "    grad_y_block_ptr = tl.make_block_ptr(\n",
    "        grad_y_ptr,\n",
    "        shape=(X_ROW,),\n",
    "        strides=(stride_grad_y_ROW,),\n",
    "        offsets=(rowtile_idx*TILE_ROW, 0),\n",
    "        block_shape=(TILE_ROW),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    # 2D matrix\n",
    "    X_block_ptr = tl.make_block_ptr(\n",
    "        X_ptr,\n",
    "        shape=(X_ROW, X_D),\n",
    "        strides=(stride_X_ROW, stride_X_D),\n",
    "        offsets=(rowtile_idx*X_ROW, 0),\n",
    "        block_shape=(X_ROW, X_D),\n",
    "        order=(1, 0)\n",
    "    )\n",
    "\n",
    "    # 1D vector\n",
    "    w_block_ptr = tl.make_block_ptr(\n",
    "        w_ptr,\n",
    "        shape=(X_D,),\n",
    "        strides=(stride_w_D,),\n",
    "        offsets=(0,)\n",
    "        block_shape=(TILE_D,)\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    # 2D matrix\n",
    "    grad_X_block_ptr = tl.make_block_ptr(\n",
    "        grad_X_ptr,\n",
    "        shape=(X_ROW, X_D),\n",
    "        strides=(stride_grad_X_ROW, stride_grad_X_D),\n",
    "        offsets=(rowtile_idx*TILE_ROW, 0),\n",
    "        block_shape=(TILE_ROW, TILE_D),\n",
    "        order=(1,0)\n",
    "    )\n",
    "\n",
    "    # A partially computed gradient, not sum() reduced yet\n",
    "    tile_grad_w_block_ptr = tl.make_block_ptr(\n",
    "        tile_grad_w_ptr,\n",
    "        shape=(n_rowtile, X_D,),\n",
    "        strides=(stride_tile_grad_w_ROW, stride_tile_grad_w_D),\n",
    "        offsets=(rowtile_idx, 0),\n",
    "        block_shape=(1, TILE_D),\n",
    "        order=(1,0)\n",
    "    )\n",
    "\n",
    "    # Sweeping across COLUMNS\n",
    "    for i in range(tl.cdiv(X_D, TILE_D)):\n",
    "        grad_y = tl.load(grad_y_block_ptr, boundary_check=(0,), padding_option=\"zero\") # (X_D,)\n",
    "        w = tl.load(w_ptr, boundary_check=(0,), padding_option=\"zero\")  # (TILE_D)\n",
    "\n",
    "        # Compute dL/dX = outer_prod(grad_y, w) = [nx1][1xD]\n",
    "        grad_X = grad_y[:,None] * w[None, :]\n",
    "        tl.store(grad_X_block_ptr, grad_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02969b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
